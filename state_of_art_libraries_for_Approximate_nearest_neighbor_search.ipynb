{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "state of art libraries for Approximate nearest neighbor search.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMQMGE5uHrpEgfXLBuPUGmF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saniagonsalves/DM_ANN/blob/main/state_of_art_libraries_for_Approximate_nearest_neighbor_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvsnix2vjDeW"
      },
      "source": [
        "**1.LSH**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcafX0zHyj9Y"
      },
      "source": [
        "**To make recommendations on conference papers by using LSH to quickly query all of the known conference papers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d9cL4ux8Afe",
        "outputId": "d07bd4eb-7566-4ff6-b335-3b841afdc54e"
      },
      "source": [
        "!pip install datasketch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasketch in /usr/local/lib/python3.7/dist-packages (1.5.3)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from datasketch) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NmYfgmDly6F",
        "outputId": "738fe502-293c-4c19-f649-1840c3b581f5"
      },
      "source": [
        "pip install --upgrade pandas"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ReJV_F5nKXt"
      },
      "source": [
        "import pandas.core.indexes as i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSMgh4mnl60B",
        "outputId": "80088181-a799-4e1c-ab0b-6af14b7986cc"
      },
      "source": [
        "!pip install apache_beam\n",
        "!pip install 'scikit_learn~=0.23.0'  \n",
        "!pip install annoy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: apache_beam in /usr/local/lib/python3.7/dist-packages (2.34.0)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (1.7)\n",
            "Requirement already satisfied: grpcio<2,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (1.42.0)\n",
            "Requirement already satisfied: fastavro<2,>=0.21.4 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (1.4.7)\n",
            "Requirement already satisfied: protobuf<4,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (3.17.3)\n",
            "Requirement already satisfied: avro-python3!=1.9.2,<1.10.0,>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (1.9.2.1)\n",
            "Requirement already satisfied: future<1.0.0,>=0.18.2 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (0.18.2)\n",
            "Requirement already satisfied: typing-extensions<4,>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (3.10.0.2)\n",
            "Requirement already satisfied: oauth2client<5,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (4.1.3)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (2018.9)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (1.3.0)\n",
            "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (0.3.1.1)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (2.8.2)\n",
            "Requirement already satisfied: numpy<1.21.0,>=1.14.3 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (1.19.5)\n",
            "Requirement already satisfied: httplib2<0.20.0,>=0.8 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (0.17.4)\n",
            "Requirement already satisfied: pyarrow<6.0.0,>=0.15.1 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (3.0.0)\n",
            "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (2.6.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (2.26.0)\n",
            "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (3.12.1)\n",
            "Requirement already satisfied: orjson<4.0 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (3.6.4)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio<2,>=1.29.0->apache_beam) (1.15.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache_beam) (0.6.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5,>=2.0.1->apache_beam) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5,>=2.0.1->apache_beam) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5,>=2.0.1->apache_beam) (4.7.2)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.7/dist-packages (from pydot<2,>=1.2.0->apache_beam) (3.0.6)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache_beam) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2.10)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2021.10.8)\n",
            "Requirement already satisfied: scikit_learn~=0.23.0 in /usr/local/lib/python3.7/dist-packages (0.23.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit_learn~=0.23.0) (3.0.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit_learn~=0.23.0) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn~=0.23.0) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit_learn~=0.23.0) (1.19.5)\n",
            "Requirement already satisfied: annoy in /usr/local/lib/python3.7/dist-packages (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgTUvKSdOo6O"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "from datasketch import MinHash, MinHashLSHForest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lf4W6vJczAtP"
      },
      "source": [
        "**Data Preprocessing**\n",
        "\n",
        "*   Remove all punctuation.\n",
        "*   Lowercase all text.\n",
        "*   Create unigram shingles (tokens) by separating any white space.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-oUO2WTi7MY"
      },
      "source": [
        "#Preprocess will split a string of text into individual tokens/shingles based on whitespace.\n",
        "def preprocess(text):\n",
        "    text = re.sub(r'[^\\w\\s]','',text)\n",
        "    tokens = text.lower()\n",
        "    tokens = tokens.split()\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60tt1gMBzrsP"
      },
      "source": [
        "example of the preprocessing step below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vu1s8xvljAra",
        "outputId": "e06e34f3-6d37-46c1-b91f-15e88096aa6b"
      },
      "source": [
        "text = 'The devil went down to California'\n",
        "print('The shingles (tokens) are:', preprocess(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shingles (tokens) are: ['the', 'devil', 'went', 'down', 'to', 'california']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oHPpblvz632"
      },
      "source": [
        "we will use the standard number of permutations of 128. We will also start by just making one recommendation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Euun1oFbjI-K"
      },
      "source": [
        "#Number of Permutations\n",
        "permutations = 128\n",
        "\n",
        "#Number of Recommendations to return\n",
        "num_recommendations = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psJg9iZ40WhG"
      },
      "source": [
        "To create the Minhash Forest\n",
        "\n",
        "\n",
        "*   Pass in a dataframe with every string to query\n",
        "*   Preprocess a string of text using the preprocessing step above\n",
        "*   Set the number of permutations in the MinHash\n",
        "*   MinHash the string on all of the shingles in the string\n",
        "*   Build a forest of all the MinHashed strings\n",
        "*   Index the forest to make it searchable\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHThbc5zjM_-"
      },
      "source": [
        "def get_forest(data, perms):\n",
        "    start_time = time.time()\n",
        "    \n",
        "    minhash = []\n",
        "    \n",
        "    for text in data['text']:\n",
        "        tokens = preprocess(text)\n",
        "        m = MinHash(num_perm=perms)\n",
        "        for s in tokens:\n",
        "            m.update(s.encode('utf8'))\n",
        "        minhash.append(m)\n",
        "        \n",
        "    forest = MinHashLSHForest(num_perm=perms)\n",
        "    \n",
        "    for i,m in enumerate(minhash):\n",
        "        forest.add(i,m)\n",
        "        \n",
        "    forest.index()\n",
        "    \n",
        "    print('It took %s seconds to build forest.' %(time.time()-start_time))\n",
        "    \n",
        "    return forest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-IJe-pC1j29"
      },
      "source": [
        "\n",
        "* Preprocess the text into shingles.\n",
        "* Set the same number of permutations for the MinHash that was used to build the forest.\n",
        "* Create the MinHash on the text using all the shingles.\n",
        "* Query the forest with MinHash and return the number of requested recommendations.\n",
        "* Provide the titles of each conference paper recommended."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW2XVMeVjQ9v"
      },
      "source": [
        "def predict(text, database, perms, num_results, forest):\n",
        "    start_time = time.time()\n",
        "    \n",
        "    tokens = preprocess(text)\n",
        "    m = MinHash(num_perm=perms)\n",
        "    for s in tokens:\n",
        "        m.update(s.encode('utf8'))\n",
        "        \n",
        "    idx_array = np.array(forest.query(m, num_results))\n",
        "    if len(idx_array) == 0:\n",
        "        return None # if your query is empty, return none\n",
        "    \n",
        "    result = database.iloc[idx_array]['title']\n",
        "    \n",
        "    print('It took %s seconds to query forest.' %(time.time()-start_time))\n",
        "    \n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZD3agTF2nCK"
      },
      "source": [
        "* load the CSV containing all the conference papers and create a new field that combines the title and the abstract into one field, so that shingles using both title and abstract are built.\n",
        "* query any string of text such as a title or general topic to return a list of recommendations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2ohuX6ojWIO",
        "outputId": "0564480d-df28-4947-d53a-89bbd0973e7c"
      },
      "source": [
        "db = pd.read_csv('/content/papers.csv')\n",
        "db['text'] = db['title'] + ' ' + db['abstract']\n",
        "forest = get_forest(db, permutations)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It took 20.31286120414734 seconds to build forest.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaxPFwaclD8W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8ed2f85-2133-40e9-9803-a2181cf04c61"
      },
      "source": [
        "num_recommendations = 10\n",
        "title = 'Using a neural net to instantiate a deformable model'\n",
        "result = predict(title, db, permutations, num_recommendations, forest)\n",
        "print('\\n Top Recommendation(s) is(are) \\n', result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It took 0.01011204719543457 seconds to query forest.\n",
            "\n",
            " Top Recommendation(s) is(are) \n",
            " 450     Speech Production Using A Neural Network with ...\n",
            "995     Neural Network Weight Matrix Synthesis Using O...\n",
            "5       Using a neural net to instantiate a deformable...\n",
            "5191    A Self-Organizing Integrated Segmentation and ...\n",
            "7       ICEG Morphology Classification using an Analog...\n",
            "3056    Proximity Effect Corrections in Electron Beam ...\n",
            "112     Adaptive Neural Networks Using MOS Charge Storage\n",
            "2069    Analytic Solutions to the Formation of Feature...\n",
            "7094    Non-Intrusive Gaze Tracking Using Artificial N...\n",
            "2457    Inferring Neural Firing Rates from Spike Train...\n",
            "Name: title, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugAe-UsdPSgf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c62ab3c2-b31f-4097-8e23-db9504b07a45"
      },
      "source": [
        "!pip install faiss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss\n",
            "  Downloading faiss-1.5.3-cp37-cp37m-manylinux1_x86_64.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from faiss) (1.19.5)\n",
            "Installing collected packages: faiss\n",
            "Successfully installed faiss-1.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9h7lRWfGZhg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a0ebdc8-f299-4bc1-b620-b84d5078b8ae"
      },
      "source": [
        "!pip install lightfm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lightfm\n",
            "  Downloading lightfm-1.16.tar.gz (310 kB)\n",
            "\u001b[K     |████████████████████████████████| 310 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lightfm) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from lightfm) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from lightfm) (2.26.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from lightfm) (0.23.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (2.0.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (2021.10.8)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightfm) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightfm) (1.1.0)\n",
            "Building wheels for collected packages: lightfm\n",
            "  Building wheel for lightfm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lightfm: filename=lightfm-1.16-cp37-cp37m-linux_x86_64.whl size=705334 sha256=19acb548830ca22e8c7b23a7ecfb9fe3d02aded3cb6ca4265271641ce2caa61a\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/56/28/5772a3bd3413d65f03aa452190b00898b680b10028a1021914\n",
            "Successfully built lightfm\n",
            "Installing collected packages: lightfm\n",
            "Successfully installed lightfm-1.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsrkclcjGD3h"
      },
      "source": [
        "from lightfm import LightFM\n",
        "from lightfm.datasets import fetch_movielens\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fgaToyJGq6N"
      },
      "source": [
        "movielens = fetch_movielens()\n",
        "train = movielens['train']\n",
        "test = movielens['test']\n",
        "\n",
        "model = LightFM(learning_rate=0.05, loss='warp', no_components=64, item_alpha=0.001)\n",
        "model.fit_partial(train, item_features=movielens['item_features'], epochs=20 )\n",
        "\n",
        "item_vectors = movielens['item_features'] * model.item_embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyUHtaMMG1Ca"
      },
      "source": [
        "with open('movies.pickle', 'wb') as f:\n",
        "    pickle.dump({\"name\": movielens['item_labels'], \"vector\": item_vectors}, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJyq8NphHkwb"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANo-lXM9HqF7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "136809ee-cb34-47c8-bbcd-3d8df8cf4cef"
      },
      "source": [
        "def load_data():\n",
        "    with open('movies.pickle', 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return data\n",
        "\n",
        "data = load_data()\n",
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': array(['Toy Story (1995)', 'GoldenEye (1995)', 'Four Rooms (1995)', ...,\n",
              "        'Sliding Doors (1998)', 'You So Crazy (1994)',\n",
              "        'Scream of Stone (Schrei aus Stein) (1991)'], dtype=object),\n",
              " 'vector': array([[ 0.39782977,  0.11020264,  0.1589934 , ..., -0.11217553,\n",
              "          0.07283162, -0.03622515],\n",
              "        [ 0.02751156, -0.12293395, -0.41641998, ...,  0.01213327,\n",
              "          0.10646569,  0.12970205],\n",
              "        [ 0.21022919,  0.05421719,  0.17373037, ...,  0.263918  ,\n",
              "          0.10492586, -0.12712367],\n",
              "        ...,\n",
              "        [-0.12282974,  0.09805849, -0.10066735, ...,  0.03841189,\n",
              "         -0.05532929,  0.02093315],\n",
              "        [-0.07288302, -0.1007875 ,  0.02831026, ..., -0.10820378,\n",
              "         -0.13741872, -0.14620647],\n",
              "        [-0.01425009,  0.0023182 , -0.09769698, ..., -0.02008126,\n",
              "         -0.14971486,  0.04387938]], dtype=float32)}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8N1MFc2Y6LE"
      },
      "source": [
        "**2)Exhaustive Search**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ov9a-BgSIhFu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a4bff32-3476-4f7d-afdf-fbb37edd7a64"
      },
      "source": [
        "!pip install faiss-gpu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.1.post2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 89.7 MB 6.7 kB/s \n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.1.post2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lqa48Y9WIY_6"
      },
      "source": [
        "import faiss\n",
        "import pickle\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R9XanG5IzjZ"
      },
      "source": [
        "class BruteForceIndex():\n",
        "    def __init__(self, vectors, labels):\n",
        "        self.vectors = vectors.astype('float32')\n",
        "        self.labels = labels\n",
        "        self.index = faiss.IndexFlatL2(vectors.shape[1])\n",
        "        self.index.add(self.vectors)\n",
        "        \n",
        "    def query(self, vectors, k=10):\n",
        "        distances, indices = self.index.search(vectors, k) \n",
        "        return [self.labels[i] for i in indices[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHh1HgRsrfQK"
      },
      "source": [
        "index = BruteForceIndex(data[\"vector\"], data[\"name\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uonyRbuhJUsX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2833ab8-86f3-42e3-a2fe-3ce6ae8f6d7a"
      },
      "source": [
        "movie_vector, movie_name = data['vector'][90:91], data['name'][90]\n",
        "simlar_movies_names = '\\n* '.join(index.query(movie_vector))\n",
        "print(f\"The most similar movies to {movie_name} are:\\n* {simlar_movies_names}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most similar movies to Nightmare Before Christmas, The (1993) are:\n",
            "* Nightmare Before Christmas, The (1993)\n",
            "* Akira (1988)\n",
            "* Bram Stoker's Dracula (1992)\n",
            "* Heavy Metal (1981)\n",
            "* Sword in the Stone, The (1963)\n",
            "* 20,000 Leagues Under the Sea (1954)\n",
            "* Fantasia (1940)\n",
            "* Casper (1995)\n",
            "* Ghost in the Shell (Kokaku kidotai) (1995)\n",
            "* Sound of Music, The (1965)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmHlvKd2ZHxz"
      },
      "source": [
        "**3) product quantization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I328w6KOQ-_m"
      },
      "source": [
        "class IVPQIndex():\n",
        "    def __init__(self, vectors, labels):\n",
        "        self.dimention = vectors.shape[1]\n",
        "        self.vectors = vectors.astype('float32')\n",
        "        self.labels = labels\n",
        "\n",
        "\n",
        "    def build(self, number_of_partition=8, search_in_x_partitions=2, subvector_size=8):\n",
        "        quantizer = faiss.IndexFlatL2(self.dimention)\n",
        "        self.index = faiss.IndexIVFPQ(quantizer, \n",
        "                                      self.dimention, \n",
        "                                      number_of_partition, \n",
        "                                      search_in_x_partitions, \n",
        "                                      subvector_size)\n",
        "        self.index.train(self.vectors)\n",
        "        self.index.add(self.vectors)\n",
        "        \n",
        "    def query(self, vectors, k=10):\n",
        "        distances, indices = self.index.search(vectors, k) \n",
        "        return [self.labels[i] for i in indices[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4swyM8s0RCNc"
      },
      "source": [
        "index = IVPQIndex(data[\"vector\"], data[\"name\"])\n",
        "index.build()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj1ppZrARFTy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a990d7f3-09c7-4d08-d2c0-1c02290bd67f"
      },
      "source": [
        "movie_index = 90\n",
        "movie_vector = data['vector'][movie_index:movie_index+1]\n",
        "print(f\"The most simillar movies to {data['name'][movie_index]} are:\")\n",
        "index.query(movie_vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most simillar movies to Nightmare Before Christmas, The (1993) are:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Nightmare Before Christmas, The (1993)',\n",
              " 'Pink Floyd - The Wall (1982)',\n",
              " 'Aladdin (1992)',\n",
              " 'Mary Poppins (1964)',\n",
              " 'Fox and the Hound, The (1981)',\n",
              " 'Fantasia (1940)',\n",
              " '20,000 Leagues Under the Sea (1954)',\n",
              " 'Sword in the Stone, The (1963)',\n",
              " 'Sound of Music, The (1965)',\n",
              " 'Old Yeller (1957)']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-Te816nZPf5"
      },
      "source": [
        "**4) trees and graphs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11CQRBgNPQvM"
      },
      "source": [
        "import annoy\n",
        "class AnnoyIndex():\n",
        "    def __init__(self, vectors, labels):\n",
        "        self.dimention = vectors.shape[1]\n",
        "        self.vectors = vectors.astype('float32')\n",
        "        self.labels = labels\n",
        "\n",
        "\n",
        "    def build(self, number_of_trees=5):\n",
        "        self.index = annoy.AnnoyIndex(self.dimention)\n",
        "        for i, vec in enumerate(self.vectors):\n",
        "            self.index.add_item(i, vec.tolist())\n",
        "        self.index.build(number_of_trees)\n",
        "        \n",
        "    def query(self, vector, k=10):\n",
        "        indices = self.index.get_nns_by_vector(vector.tolist(), k)\n",
        "        return [self.labels[i] for i in indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Js7yJoswPYo6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c038a19b-fdfc-40a8-ed40-57a083ad8254"
      },
      "source": [
        "index = AnnoyIndex(data[\"vector\"], data[\"name\"])\n",
        "index.build()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: FutureWarning: The default argument for metric will be removed in future version of Annoy. Please pass metric='angular' explicitly.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uvwB82IPduh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "866e2054-40b3-4489-9cf3-ee293bc8528a"
      },
      "source": [
        "movie_vector, movie_name = data['vector'][90], data['name'][90]\n",
        "simlar_movies_names = '\\n* '.join(index.query(movie_vector))\n",
        "print(f\"The most similar movies to {movie_name} are:\\n* {simlar_movies_names}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most similar movies to Nightmare Before Christmas, The (1993) are:\n",
            "* Nightmare Before Christmas, The (1993)\n",
            "* Fantasia (1940)\n",
            "* Heavy Metal (1981)\n",
            "* Beauty and the Beast (1991)\n",
            "* Lion King, The (1994)\n",
            "* Star Trek: The Wrath of Khan (1982)\n",
            "* Jurassic Park (1993)\n",
            "* Batman (1989)\n",
            "* Aladdin (1992)\n",
            "* E.T. the Extra-Terrestrial (1982)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZDhLQyvWCSm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f955d68-c9db-4b1b-ad30-72a77d6918b6"
      },
      "source": [
        "!pip install nmslib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nmslib\n",
            "  Downloading nmslib-2.1.1-cp37-cp37m-manylinux2010_x86_64.whl (13.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.5 MB 75 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from nmslib) (1.19.5)\n",
            "Collecting pybind11<2.6.2\n",
            "  Downloading pybind11-2.6.1-py2.py3-none-any.whl (188 kB)\n",
            "\u001b[K     |████████████████████████████████| 188 kB 49.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from nmslib) (5.4.8)\n",
            "Installing collected packages: pybind11, nmslib\n",
            "Successfully installed nmslib-2.1.1 pybind11-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1P54opKVjBd"
      },
      "source": [
        "import nmslib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhxL04EsUo5P"
      },
      "source": [
        "class NMSLIBIndex():\n",
        "    def __init__(self, vectors, labels):\n",
        "        self.dimention = vectors.shape[1]\n",
        "        self.vectors = vectors.astype('float32')\n",
        "        self.labels = labels\n",
        "\n",
        "    def build(self):\n",
        "        self.index = nmslib.init(method='hnsw', space='cosinesimil')\n",
        "        self.index.addDataPointBatch(self.vectors)\n",
        "        self.index.createIndex({'post': 2})\n",
        "        \n",
        "    def query(self, vector, k=10):\n",
        "        indices = self.index.knnQuery(vector, k=k)\n",
        "        return [self.labels[i] for i in indices[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr94pbDCUt3u"
      },
      "source": [
        "index = NMSLIBIndex(data[\"vector\"], data[\"name\"])\n",
        "index.build()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag3Vss8XU0rs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b9674ab-3743-4201-b20f-bbcca262467e"
      },
      "source": [
        "movie_vector, movie_name = data['vector'][90], data['name'][90]\n",
        "simlar_movies_names = '\\n* '.join(index.query(movie_vector))\n",
        "print(f\"The most similar movies to {movie_name} are:\\n* {simlar_movies_names}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most similar movies to Nightmare Before Christmas, The (1993) are:\n",
            "* Nightmare Before Christmas, The (1993)\n",
            "* Bram Stoker's Dracula (1992)\n",
            "* Fantasia (1940)\n",
            "* Heavy Metal (1981)\n",
            "* Beauty and the Beast (1991)\n",
            "* Lion King, The (1994)\n",
            "* Star Trek: The Wrath of Khan (1982)\n",
            "* Akira (1988)\n",
            "* Jurassic Park (1993)\n",
            "* Sound of Music, The (1965)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHAaLv_dZXX1"
      },
      "source": [
        "**5) hnsw**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-NkAZhEYDoq"
      },
      "source": [
        "import shutil\n",
        "import urllib.request as request\n",
        "from contextlib import closing\n",
        "\n",
        "# first we download the Sift1M dataset\n",
        "with closing(request.urlopen('ftp://ftp.irisa.fr/local/texmex/corpus/sift.tar.gz')) as r:\n",
        "    with open('sift.tar.gz', 'wb') as f:\n",
        "        shutil.copyfileobj(r, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jIlgjDcYJHv"
      },
      "source": [
        "import tarfile\n",
        "\n",
        "# the download leaves us with a tar.gz file, we unzip it\n",
        "tar = tarfile.open('sift.tar.gz', \"r:gz\")\n",
        "tar.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9moYJcVeYOUL"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# now define a function to read the fvecs file format of Sift1M dataset\n",
        "def read_fvecs(fp):\n",
        "    a = np.fromfile(fp, dtype='int32')\n",
        "    d = a[0]\n",
        "    return a.reshape(-1, d + 1)[:, 1:].copy().view('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbcCCwcRYRdn"
      },
      "source": [
        "# data we will search through\n",
        "wb = read_fvecs('/content/sift_base.fvecs')  # 1M samples\n",
        "# also get some query vectors to search with\n",
        "xq = read_fvecs('/content/sift_query.fvecs')\n",
        "# take just one query (there are many in sift_learn.fvecs)\n",
        "xq = xq[0].reshape(1, xq.shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omGdAT-jbSCe"
      },
      "source": [
        "**To explore the implementation of HNSW in Facebook AI Similarity Search (Faiss)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RHk4ZFNXgcQ"
      },
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# now define a function to read the fvecs file format of Sift1M dataset\n",
        "def read_fvecs(fp):\n",
        "    a = np.fromfile(fp, dtype='int32')\n",
        "    d = a[0]\n",
        "    return a.reshape(-1, d + 1)[:, 1:].copy().view('float32')\n",
        "\n",
        "# 1M samples\n",
        "xb = read_fvecs('/content/sift_base.fvecs')\n",
        "# queries\n",
        "xq = read_fvecs('/content/sift_query.fvecs')[0].reshape(1, -1)\n",
        "xq_full = read_fvecs('/content/sift_query.fvecs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCtWiHVicSkJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c71e6ea7-c9d0-421c-e541-10e9366c0e1f"
      },
      "source": [
        "xq.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 128)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrmTsGzrcVhw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11d8e5ab-9600-4a86-faab-dcccfa27ba0f"
      },
      "source": [
        "wb.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000000, 128)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TmX1DfjcsrY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "571bd815-ef3f-473b-9c30-8f9267b944f2"
      },
      "source": [
        "# setup our HNSW parameters\n",
        "d = 128  # vector size\n",
        "M = 32\n",
        "efSearch = 32  # number of entry points (neighbors) we use on each layer\n",
        "efConstruction = 32  # number of entry points used on each layer\n",
        "                     # during construction\n",
        "\n",
        "index = faiss.IndexHNSWFlat(d, M)\n",
        "print(index.hnsw)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<faiss.swigfaiss.HNSW; proxy of <Swig Object of type 'faiss::HNSW *' at 0x7ff2f9ba63f0> >\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W70dkcE0bdJU"
      },
      "source": [
        "Before building the index with index.add the HNSW structure is empty:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL5sOmTDc3dE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9456fa9-6d45-40ab-900b-f0752213ab28"
      },
      "source": [
        "# the HNSW index starts with no levels\n",
        "index.hnsw.max_level"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9EMK1tCc7XF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5b0f779-5a92-478d-a8db-4aa12d0f1c20"
      },
      "source": [
        "# and levels (or layers) are empty too\n",
        "levels = faiss.vector_to_array(index.hnsw.levels)\n",
        "np.bincount(levels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([], dtype=int64)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB0YKGjjee_B"
      },
      "source": [
        "We can set the efConstruction and efSearch parameters, only efConstruction must be set before building the index. efSearch only affects search time behavior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcbIauQkc-3U"
      },
      "source": [
        "index.hnsw.efConstruction = efConstruction\n",
        "index.hnsw.efSearch = efSearch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emNIFne4dD_b"
      },
      "source": [
        "index.add(xb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADWIeJagejnK"
      },
      "source": [
        "Now that we have added our data (and built the index) we will see that the HNSW structure has been populated.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBzSU27vfirA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b33c477-0bdc-4f85-e493-ccea21dfce5a"
      },
      "source": [
        "# after adding our data we will find that the level\n",
        "# has been set automatically\n",
        "index.hnsw.max_level"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjAfstjufmft",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7a206e8-9780-4265-d5e2-77f1c1022f25"
      },
      "source": [
        "# and levels (or layers) are now populated\n",
        "levels = faiss.vector_to_array(index.hnsw.levels)\n",
        "np.bincount(levels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([     0, 968746,  30276,    951,     26,      1])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKUWcg_0fpki",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4083618b-422b-4755-b774-cb14e111dff0"
      },
      "source": [
        "index.hnsw.entry_point"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "118295"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCnG6hsomP0U"
      },
      "source": [
        "The HNSW:set_default_probas function (from HNSW.cpp)calculates the number of neighbors (in total) a vertex will have across the calculated number of layers. We find that Faiss' implementation does not use M_max or M_max0 directly, but instead uses M to set these values. M_max is set to M, and M_max is set to 2*M."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkjSPuGLfvLm"
      },
      "source": [
        "def set_default_probas(M: int, m_L: float):\n",
        "    nn = 0  # set nearest neighbors count = 0\n",
        "    cum_nneighbor_per_level = []\n",
        "    level = 0  # we start at level 0\n",
        "    assign_probas = []\n",
        "    while True:\n",
        "        # calculate probability for current level\n",
        "        proba = np.exp(-level / m_L) * (1 - np.exp(-1 / m_L))\n",
        "        # once we reach low prob threshold, we've created enough levels\n",
        "        if proba < 1e-9: break\n",
        "        assign_probas.append(proba)\n",
        "        # neighbors is == M on every level except level 0 where == M*2\n",
        "        nn += M*2 if level == 0 else M\n",
        "        cum_nneighbor_per_level.append(nn)\n",
        "        level += 1\n",
        "    return assign_probas, cum_nneighbor_per_level"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEvYMIsFfyKi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46e29fe4-f693-4109-fe20-f7450bbbacf0"
      },
      "source": [
        "assign_probas, cum_nneighbor_per_level = set_default_probas(\n",
        "    32, 1/np.log(32)\n",
        ")\n",
        "assign_probas, cum_nneighbor_per_level"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0.96875,\n",
              "  0.030273437499999986,\n",
              "  0.0009460449218749991,\n",
              "  2.956390380859371e-05,\n",
              "  9.23871994018553e-07,\n",
              "  2.887099981307982e-08],\n",
              " [64, 96, 128, 160, 192, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q16Stf14f1LQ"
      },
      "source": [
        "# this is copy of HNSW::random_level function\n",
        "def random_level(assign_probas: list, rng):\n",
        "    # get random float from 'r'andom 'n'umber 'g'enerator\n",
        "    f = rng.uniform() \n",
        "    for level in range(len(assign_probas)):\n",
        "        # if the random float is less than level probability...\n",
        "        if f < assign_probas[level]:\n",
        "            # ... we assert at this level\n",
        "            return level\n",
        "        # otherwise subtract level probability and try again\n",
        "        f -= assign_probas[level]\n",
        "    # below happens with very low probability\n",
        "    return len(assign_probas) - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xK9RdVRf49-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97d621c8-1657-492a-e1d6-34232510c1db"
      },
      "source": [
        "chosen_levels = []\n",
        "rng = np.random.default_rng(12345)\n",
        "for _ in range(1_000_000):\n",
        "    chosen_levels.append(random_level(assign_probas, rng))\n",
        "np.bincount(chosen_levels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([968821,  30170,    985,     23,      1])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HySzGYErf_J9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b120a709-8510-4db1-a8e0-94d82d60aaf6"
      },
      "source": [
        "1/np.log(32)  # the previous value we used for m_L"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.28853900817779266"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQdawnxtgCz0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94c748ed-d699-4591-c5a4-51aa54c2912b"
      },
      "source": [
        "set_default_probas(32, 0.09)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0.9999850546614752, 1.4945115161637832e-05], [64, 96])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfkGOHXAgFtx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52c95aac-c2da-42df-e8f9-8abd81e0e892"
      },
      "source": [
        "levels = faiss.vector_to_array(index.hnsw.levels)\n",
        "np.bincount(levels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([     0, 968746,  30276,    951,     26,      1])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5diUu7xYgIdW"
      },
      "source": [
        "del index\n",
        "index = faiss.IndexHNSWFlat(d, 32)\n",
        "index.hnsw.set_default_probas(32, 0.09)  # HNSW::set_default_probas(int M, float levelMult)\n",
        "index.hnsw.efConstruction = efConstruction\n",
        "index.add(xb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzCzZV6WhkLE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cc18997-c31f-4c5d-94bc-44937993018f"
      },
      "source": [
        "levels = faiss.vector_to_array(index.hnsw.levels)\n",
        "np.bincount(levels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([     0, 968746,  30276,    951,     26,      1])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5y8KVBhmmJY"
      },
      "source": [
        "Finally, let's validate that m_L values ~0 produce a single layer HNSW graph "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgq8z4YdhneW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f63ca6e-96c1-4d6a-a070-f4d82a8343b2"
      },
      "source": [
        "assign_probas, cum_nneighbor_per_level = set_default_probas(32, 0.0000001)\n",
        "assign_probas, cum_nneighbor_per_level"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([1.0], [64])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1ZmpJC9hqgY"
      },
      "source": [
        "chosen_levels = []\n",
        "rng = np.random.default_rng(12345)\n",
        "for _ in range(1_000_000):\n",
        "    chosen_levels.append(random_level(assign_probas, rng))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWYSsmKohuup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a42e359a-293a-42cd-f18f-e8cd4736313d"
      },
      "source": [
        "np.bincount(chosen_levels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1000000])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JcThwOPmsBr"
      },
      "source": [
        "Faiss also always ensures that at least one vertex is included at the highest level, as we can see by creating a small index:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGmNcO3jhx9d"
      },
      "source": [
        "del index\n",
        "index = faiss.IndexHNSWFlat(d, 32)\n",
        "index.hnsw.efConstruction = efConstruction\n",
        "index.add(xb[:1_000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ty2dE24Wh0sq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b1e934b-bdd4-43c9-b8e2-a6b0aedcd096"
      },
      "source": [
        "levels = faiss.vector_to_array(index.hnsw.levels)\n",
        "np.bincount(levels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0, 974,  25,   1])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    }
  ]
}